{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "governing-agency",
   "metadata": {},
   "source": [
    "# Experiment 1: Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nutritional-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.6.0\n",
      "This notebook use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from glob import glob\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# data processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "\n",
    "# device setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'This notebook use {device}')\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nutritional-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 사용자 정의\n",
    "class path:\n",
    "    train = '../input/data/train'\n",
    "    train_img = f'{train}/images'\n",
    "    train_df = f'{train}/train.csv'\n",
    "    test = '../input/data/eval'\n",
    "    test_img = f'{test}/images'\n",
    "    test_df = f'{test}/info.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sophisticated-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "NUM_WORKERS=2\n",
    "LEARNING_RATE=1e-4\n",
    "EPOCHS=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-guinea",
   "metadata": {},
   "source": [
    "## Augmentation Methods\n",
    "\n",
    "- RandomHorizontalFlip\n",
    "- CenterCrop\n",
    "- RandomCrop\n",
    "- Resize\n",
    "- Normalize: ToTensor() 이후에 사용\n",
    "- Color Jitter - Brightness\n",
    "- Color Jitter - Contrast\n",
    "- Color Jitter - Saturation\n",
    "- ToTensor\n",
    "- GrayScale\n",
    "- FiveCrop, TenCrop\n",
    "- GaussianBlur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-outline",
   "metadata": {},
   "source": [
    "## Augmentation 1\n",
    "- CenterCrop (224, 224)\n",
    "- RandomHorizontalFlip\n",
    "- ShiftScaleRotate\n",
    "- Brightness\n",
    "- Contrast\n",
    "- Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fleet-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(224, 224), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = transforms.Compose([\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            RandomChoice([transforms.RandomHorizontalFlip(p=1),\n",
    "                         transforms.RandomRotation([-8, +8])]),\n",
    "            ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "            AddGaussianNoise(0., 1.)\n",
    "        ])\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = transforms.Compose([\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-quarter",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "million-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset 정의\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_data = self.data.iloc[idx]\n",
    "        target = current_data.target\n",
    "        img = Image.open(current_data.path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "boring-brother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset : 18900\n",
      "Shape of image : torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "dataset = MaskDataset(f'{path.train}/train_modified.csv', transform['train'])\n",
    "print('Size of dataset :', len(dataset))\n",
    "print('Shape of image :', dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "chicken-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(dataset)*0.8)\n",
    "n_val = len(dataset)-n_train\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "athletic-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-hamilton",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "hourly-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "apparent-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b3', num_classes=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "emerging-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, weight in model.named_parameters():\n",
    "    weight.requires_grad = False\n",
    "model._fc.weight.requires_grad = True\n",
    "model._fc.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "alpine-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "specific-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "isolated-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model, data_iter, batch_size):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        for batch_img, batch_lab in tqdm(data_iter):\n",
    "            X = batch_img.view(-1, 3, 224, 224).float().to(device)\n",
    "            Y = batch_lab.to(device)\n",
    "            y_pred = model(X)\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            correct += (predicted == Y).sum().item()\n",
    "            total += batch_img.size(0)\n",
    "        val_acc = (100 * correct / total)\n",
    "        model.train()\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "blond-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, test, model, criterion, optimizer, print_every=1):\n",
    "    print(f\"====== Training Starts! ======\")\n",
    "    best_accr_val = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_val_sum = 0\n",
    "        for batch_img, batch_lab in tqdm(train):\n",
    "            X = batch_img.view(-1, 3, 224, 224).float().to(device)\n",
    "            Y = batch_lab.to(device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, Y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            loss_val_sum += loss\n",
    "          \n",
    "        if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)):\n",
    "            loss_val_avg = loss_val_sum / len(train)\n",
    "            accr_val = test_eval(model, test, BATCH_SIZE)\n",
    "            if accr_val > best_accr_val:\n",
    "                torch.save(model.state_dict(), f\"{path.train}/{epoch}_accuracy_{accr_val:.2f}_efficientnet.ckpt\")\n",
    "                best_accr_val = accr_val\n",
    "            print(f\"epoch:[{epoch+1}/{EPOCHS}] cost:[{loss_val_avg:.3f}] test_accuracy:[{accr_val:.3f}]\") \n",
    "\n",
    "    print(f\"====== Training Done! ======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "exact-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training Starts! ======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a3d79e8b6c429ca19f721a2cc83ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=945.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e862f2007dcd4db986ca948a3ef9f475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=237.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:[1/15] cost:[2.712] test_accuracy:[20.450]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb32f730e4340e9a4fc352cd6d7cb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=945.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-c559878a4bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-019db2ca972f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train, test, model, criterion, optimizer, print_every)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_lab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \"\"\"\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# scale drop connect_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# Head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Squeeze and Excitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_se\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mx_squeezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mx_squeezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_se_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_squeezed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mx_squeezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_squeezed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 adaptive_avg_pool2d, (input,), input, output_size)\n\u001b[1;32m    925\u001b[0m     \u001b[0m_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_with_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(train_loader, val_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name('efficientnet-b1')\n",
    "model.load_state_dict(torch.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(path.test, 'info.csv'))\n",
    "image_dir = os.path.join(path.test, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, transform['val'])\n",
    "\n",
    "loader = DataLoader(dataset, shuffle=False)\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.view(-1, 3, 384, 512).float().to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(path.test, 'submission_202104011613.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
